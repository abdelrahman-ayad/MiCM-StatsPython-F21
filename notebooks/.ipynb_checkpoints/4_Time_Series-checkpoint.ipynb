{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series analysis and ARIMA\n",
    "### The following notebook has been inspired by the following resources: \n",
    "1- https://software.intel.com/content/www/us/en/develop/training/course-time-series-analysis.html\n",
    "\n",
    "2- https://towardsdatascience.com/machine-learning-part-19-time-series-and-autoregressive-integrated-moving-average-model-arima-c1005347b0d7\n",
    "\n",
    "3- https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stationary Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for time series data to be stationary, the data must exhibit four properties over time:\n",
    "1. constant mean\n",
    "2. constant variance\n",
    "3. constant autocorrelation structure\n",
    "4. no periodic component\n",
    "\n",
    "Mean, variance, and periodic component (aka seasonality) should be familiar to you. Autocorrelation may not be. Autocorrelation simply means that the current time series measurement is correlated with a past measurement. For example, today's stock price is often highly correlated with yesterday's price. \n",
    "\n",
    "To discuss these things simply we must introduce the idea of a **lag**. Lag is a fancy term for delay. Say you wanted to know if today's stock price correlated better with yesterday's price or the price from two days ago. You could test this by computing the correlation between the original time series and the same series delayed one time interval. Therefore, the second value of the original time series would be compared with the first of the delayed. The third original value would be compared with the second of the delayed. And so on. Performing this process for a lag of 1 and a lag of 2, respectively, would yield two correlation outputs. This output would tell you which lag is more correlated. That is **autocorrelation** in a nutshell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "\n",
    "# data\n",
    "time = np.arange(100)\n",
    "stationary = np.random.normal(loc=0, scale=1.0, size=len(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data with no trend or seasonality. \n",
    "def run_sequence_plot(x, y, title, xlabel=\"time\", ylabel=\"series\"):\n",
    "    plt.plot(x, y, 'k-')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "run_sequence_plot(time, stationary, \n",
    "                  title=\"Stationary TS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed to start series\n",
    "seed = 3.14\n",
    "\n",
    "# create autocorrelated data\n",
    "lagged = np.empty_like(time, dtype='float')\n",
    "for t in time:\n",
    "    lagged[t] = seed + np.random.normal(loc=0, scale=2.5, size=1)\n",
    "    seed = lagged[t]\n",
    "    \n",
    "run_sequence_plot(time, lagged,\n",
    "                  title=\"Nonstationary Data w/Lagged Structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of non-stationary data\n",
    "trend = (time * 2.75) + stationary\n",
    "run_sequence_plot(time, trend,\n",
    "                  title=\"Nonstationary Data w/Trend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "# Heteroscedasticity: \n",
    "# a vector of random variables is heteroscedastic if the variability of\n",
    "# the random disturbance is different across elements of the vector\n",
    "level_1 = np.random.normal(loc=0, scale=1.0, size = 50)\n",
    "level_2 = np.random.normal(loc=0, scale=10.0, size = 50)\n",
    "heteroscedasticity = np.append(level_1, level_2)\n",
    "\n",
    "run_sequence_plot(time, heteroscedasticity,\n",
    "                  title=\"Nonstationary Data w/Heteroscedasticity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonsality\n",
    "seasonality = 10 + np.sin(time) * 10\n",
    "run_sequence_plot(time, seasonality,\n",
    "                  title=\"Nonstationary Data w/Seasonality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Stationarity\n",
    "\n",
    "Some common techniques used to identify if a time series is stationary or not:\n",
    "\n",
    "1. Run-sequence plots\n",
    "2. Summary statistics & histogram plots\n",
    "3. Augmented Dickey-Fuller test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2- split data into 10 chunks\n",
    "chunks = np.split(trend, indices_or_sections=10)\n",
    "# compare means and variances\n",
    "print(\"{} | {:7} | {}\".format(\"Chunk\", \"Mean\", \"Variance\"))\n",
    "print(\"-\" * 26)\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(\"{:5} | {:.6} | {:.6}\".format(i, np.mean(chunk), np.var(chunk)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same results with numpy\n",
    "print(np.mean(chunks, axis=1))\n",
    "print('----------------------')\n",
    "print(np.var(chunks, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Dickey-Fuller  (ADF) Test\n",
    "This is a statistical procedure to suss out whether a time series is stationary or not. We won't go into all the nitty gritty details but here's what you need to know:\n",
    "1. **Null hypothesis:** the series is nonstationary.\n",
    "2. **Alternative hypothesis:** the series is stationary.\n",
    "\n",
    "Like any statistical test you should set a significance level or threshold that determines whether you should accept or reject the null. \n",
    "> The value 0.05 is common but depends upons numerous factors.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "adf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(stationary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, **adf** is the value of the test statistic. \n",
    "# The more negative the value, the more confident we can be that the series is stationary. \n",
    "print(adf)\n",
    "# Here we see a value of -10. That may not mean anything to you just yet but the **pvalue** should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, **pvalue** is interpreted like any p-value. Once we set a threshold, we can compare this p-value to that threshold. Either we reject or fail to reject the null. \n",
    "# Here **pvalue** is very close to zero (~$10^{-17}$) so we reject the null that this data is nonstationary.\n",
    "print(pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nobs is the number of observations in the time series\n",
    "print(nobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the **critical_values** variable provides test statistic threholds for common significant levels.\n",
    "# Here we see a test statistic of roughly -2.89 and lower is sufficient to reject\n",
    "# the null using a significance level of 5%.\n",
    "print(critical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(trend, regression='c')\n",
    "print(\"ADF: \", adf)\n",
    "print(\"p-value:\", pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a high pvalue indicate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(lagged, regression='c')\n",
    "print(\"ADF: \", adf)\n",
    "print(\"p-value:\", pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Common Nonstationary-to-Stationary Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Trend & Seasonality with Statsmodels\n",
    "trend_seasonality = trend + seasonality + stationary\n",
    "run_sequence_plot(time, trend_seasonality,\n",
    "                  title=\"Nonstationary Data w/Trend + Seasonality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_b4, pvalue_b4, usedlag_, nobs_, critical_values_, icbest_ = adfuller(trend_seasonality)\n",
    "print(\"ADF: \", adf_b4)\n",
    "print(\"p-value: \", pvalue_b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "ss_decomposition = seasonal_decompose(x=trend_seasonality, model='additive', period=6)\n",
    "est_trend = ss_decomposition.trend\n",
    "est_seasonal = ss_decomposition.seasonal\n",
    "est_residual = ss_decomposition.resid\n",
    "\n",
    "run_sequence_plot(time, est_trend, title=\"Trend\", ylabel=\"series\")\n",
    "plt.show()\n",
    "run_sequence_plot(time, est_seasonal, title=\"Seasonality\", ylabel=\"series\")\n",
    "plt.show()\n",
    "run_sequence_plot(time, est_residual, title=\"Residuals\", ylabel=\"series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(est_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to remove the NAN values\n",
    "est_residual = est_residual[~np.isnan(est_residual)]\n",
    "print(est_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_after, pvalue_after, usedlag_, nobs_, critical_values_, icbest_ = adfuller(est_residual[3:-3])\n",
    "print(\"ADF: \", adf_after)\n",
    "print(\"p-value: \", pvalue_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2- Removing Autocorrelation with Differencing\n",
    "difference = lagged[:-1] - lagged[1:]\n",
    "run_sequence_plot(time[:-1], difference,\n",
    "                  title=\"Stationary Data After Differencing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_after, pvalue_after, usedlag_, nobs_, critical_values_, icbest_ = adfuller(difference)\n",
    "print(\"ADF: \", adf_after)\n",
    "print(\"p-value: \", pvalue_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA models\n",
    "ARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. It is a generalization of the simpler AutoRegressive Moving Average and adds the notion of integration.\n",
    "\n",
    "AR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n",
    "\n",
    "I: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n",
    "\n",
    "MA: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n",
    "Each of these components are explicitly specified in the model as a parameter. A standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used.\n",
    "\n",
    "The parameters of the ARIMA model are defined as follows:\n",
    "\n",
    "p: The number of lag observations included in the model, also called the lag order.\n",
    "\n",
    "d: The number of times that the raw observations are differenced, also called the degree of differencing.\n",
    "\n",
    "q: The size of the moving average window, also called the order of moving average.\n",
    "\n",
    "A linear regression model is constructed including the specified number and type of terms, and the data is prepared by a degree of differencing in order to make it stationary, i.e. to remove trend and seasonal structures that negatively affect the regression model.\n",
    "\n",
    "A value of 0 can be used for a parameter, which indicates to not use that element of the model. This way, the ARIMA model can be configured to perform the function of an ARMA model, and even a simple AR, I, or MA model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'https://raw.githubusercontent.com/abdelrahman-ayad/MiCM-StatsPython-F21/main/data/sales.csv'\n",
    "df = pd.read_csv(fileName)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an ARIMA model and plot residual errors\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# fit model\n",
    "model = ARIMA(df.Sales, order=(5,1,0))\n",
    "model_fit = model.fit()\n",
    "# summary of fit model\n",
    "print(model_fit.summary())\n",
    "# line plot of residuals\n",
    "residuals = pd.DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "plt.show()\n",
    "# density plot of residuals\n",
    "residuals.plot(kind='kde')\n",
    "plt.show()\n",
    "# summary stats of residuals\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using ARIMA to forecast\n",
    "The ARIMA model can be used to forecast future time steps.\n",
    "\n",
    "We can use the predict() function on the ARIMAResults object to make predictions. It accepts the index of the time steps to make predictions as arguments. These indexes are relative to the start of the training dataset used to make predictions.\n",
    "\n",
    "If we used 100 observations in the training dataset to fit the model, then the index of the next time step for making a prediction would be specified to the prediction function as start=101, end=101. This would return an array with one element containing the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "# split into train and test sets\n",
    "X = df.Sales.values\n",
    "size = int(len(X) * 0.66)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "\n",
    "\n",
    "# walk-forward validation\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(5,1,0))\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "# evaluate forecasts\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "# plot forecasts against actual outcomes\n",
    "plt.plot(test, color='blue', label = 'original')\n",
    "plt.plot(predictions, color='red', label = 'predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
